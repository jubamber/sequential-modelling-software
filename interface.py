# äº¤äº’å±‚ï¼šGradioï¼Œä¸šåŠ¡æµç¨‹çš„æ§åˆ¶å™¨ï¼ˆControllerï¼‰ã€‚å®ƒå°† UI äº‹ä»¶ä¸åé¢å‡ å±‚é€»è¾‘ä¸²è”èµ·æ¥
import glob
import os
import json

import gradio as gr
from fastapi import FastAPI
import data_processor
import model_engine
import visualizer
import utils
import config

def clean_model_history():
    """æ¸…ç†ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶"""
    save_dir = config.MODEL_SAVE_DIR
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(save_dir):
        return "### âš ï¸ ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚"
    
    # æŸ¥æ‰¾å¸¸è§åç¼€çš„æ¨¡å‹æ–‡ä»¶
    # æ ¹æ®ä½ çš„ train_model é€»è¾‘ï¼Œæ¨¡å‹ä¿å­˜ä¸º .keras (Keras) æˆ– .pkl (Sktime)
    files_to_delete = glob.glob(os.path.join(save_dir, "*.keras")) + \
                      glob.glob(os.path.join(save_dir, "*.pkl")) + \
                      glob.glob(os.path.join(save_dir, "*.json"))
    
    deleted_count = 0
    errors = []
    
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            deleted_count += 1
        except Exception as e:
            errors.append(f"æ— æ³•åˆ é™¤ {os.path.basename(file_path)}: {str(e)}")

    deleted_count = int(deleted_count / 2) if deleted_count > 0 else 0
    
    # æ„å»ºè¿”å›ä¿¡æ¯
    if len(errors) > 0:
        return f"### âš ï¸ æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {deleted_count} ä¸ªæ¨¡å‹æ–‡ä»¶ã€‚\né”™è¯¯: {'; '.join(errors)}"
    elif deleted_count == 0:
        return "### â„¹ï¸ æš‚æ— å·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶å¯æ¸…ç†ã€‚"
    else:
        return f"### âœ… æˆåŠŸæ¸…ç†å†å²æ¨¡å‹ç¼“å­˜ï¼ˆå…±åˆ é™¤ {deleted_count} ä¸ªæ¨¡å‹æ–‡ä»¶ï¼‰ã€‚"
    

def get_metadata_path(model_path):
    """æ ¹æ®æ¨¡å‹è·¯å¾„è·å–å¯¹åº”çš„ metadata.json è·¯å¾„"""
    # å‡è®¾æ¨¡å‹æ˜¯ model.kerasï¼Œå…ƒæ•°æ®å­˜ä¸º model.json
    base, _ = os.path.splitext(model_path)
    return f"{base}_meta.json"

def save_pipeline_config(save_path, params):
    """ä¿å­˜è®­ç»ƒå‚æ•°åˆ° json"""
    meta_path = get_metadata_path(save_path)
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(params, f, indent=4, ensure_ascii=False)

def load_pipeline_config(model_filename):
    """è¯»å–å·²ä¿å­˜çš„å‚æ•°"""
    save_dir = config.MODEL_SAVE_DIR
    model_path = os.path.join(save_dir, model_filename)
    meta_path = get_metadata_path(model_path)
    
    if not os.path.exists(meta_path):
        return None
    
    with open(meta_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def get_saved_model_list():
    """è·å–æ‰€æœ‰å·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶åˆ—è¡¨"""
    save_dir = config.MODEL_SAVE_DIR
    if not os.path.exists(save_dir):
        return []
    
    # è·å– .keras å’Œ .pkl æ–‡ä»¶
    files = glob.glob(os.path.join(save_dir, "*.keras")) + \
            glob.glob(os.path.join(save_dir, "*.pkl"))
    # æŒ‰æ—¶é—´å€’åºæ’åˆ—
    files.sort(key=os.path.getmtime, reverse=True)
    return [os.path.basename(f) for f in files]
    
def run_pipeline(dataset_name, use_local_data, local_file_path, clean_dataset_name, # <--- ä¿®æ”¹ï¼šæ–°å¢ use_local_data
                 model_type, epochs, batch_size, look_back, split_ratio, 
                 p, d, q, auto_arima, P, D, Q, s, 
                 use_saved_model, saved_model_name, 
                 enable_future, future_steps, progress=gr.Progress()):
    """æ§åˆ¶å™¨å‡½æ•°ï¼šåè°ƒæ•°æ®ã€æ¨¡å‹å’Œç»˜å›¾"""
    try:

        # ç¡®å®šç”¨äºä¿å­˜çš„æ–‡ä»¶åæ ‡è¯†
        # å¦‚æœæ˜¯æœ¬åœ°æ•°æ®ï¼Œç”¨ clean_dataset_name (æ–‡ä»¶å)ï¼Œå¦åˆ™ç”¨ dataset_name
        if use_local_data:
            # å¦‚æœå‹¾é€‰äº†æœ¬åœ°æ•°æ®ï¼Œä½¿ç”¨ clean_dataset_name (æ–‡ä»¶å) ä½œä¸ºæ ‡è¯†
            actual_dataset_name_for_save = clean_dataset_name 
            # ç¡®ä¿æœ‰æ–‡ä»¶è·¯å¾„
            if not local_file_path:
                return "### âŒ é”™è¯¯ï¼šè¯·ä¸Šä¼ æœ¬åœ° CSV æ–‡ä»¶ã€‚", None
        else:
            # å¦åˆ™ä½¿ç”¨é¢„ç½®æ•°æ®é›†åç§°
            actual_dataset_name_for_save = dataset_name
        
        # --- æ¨¡å¼åˆ†æ”¯ ---
        if use_saved_model and saved_model_name:
            progress(0.1, desc=f"æ­£åœ¨åŠ è½½æ¨¡å‹: {saved_model_name}...")
            save_path = os.path.join(config.MODEL_SAVE_DIR, saved_model_name)
            
            # 1. åŠ è½½æ¨¡å‹å®ä½“
            try:
                if saved_model_name.endswith(".keras"):
                    from keras.models import load_model
                    model = load_model(save_path)
                elif saved_model_name.endswith(".pkl"):
                    import joblib
                    model = joblib.load(save_path)
                else:
                    return f"### âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {saved_model_name}", None
            except Exception as e:
                return f"### âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", None
                
            file_size_str = utils.get_file_size_str(save_path)
            report_prefix = "### â™»ï¸ æ¨¡å‹åŠ è½½æŠ¥å‘Š (å·²ä¿å­˜æ¨¡å‹)"
            
        else:
            # === åŸæœ‰çš„è®­ç»ƒé€»è¾‘ ===
            
            # 1. æ•°æ®å‡†å¤‡
            progress(0, desc="åŠ è½½ä¸å¤„ç†æ•°æ®...")
            try:
                # æ ¹æ® use_local_data çš„é€»è¾‘ï¼Œload_source å¯èƒ½æ˜¯æ–‡ä»¶åä¹Ÿå¯èƒ½æ˜¯è·¯å¾„
                # data_processor.load_raw_data å†…éƒ¨åº”è¯¥å…¼å®¹ (å¦‚æœ dataset_name ä¸åœ¨é¢„ç½®åˆ—è¡¨ä¸­ä¸”æ˜¯è·¯å¾„)
                # è¿™é‡Œæˆ‘ä»¬æ˜¾å¼ä¼ å‚ä»¥é˜²ä¸‡ä¸€
                if use_local_data:
                    df = data_processor.load_raw_data("ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®", local_file_path) # å€Ÿç”¨æ—§æ¥å£é€»è¾‘æˆ–ç›´æ¥ä¼ path
                else:
                    df = data_processor.load_raw_data(dataset_name)
            except Exception as e:
                return f"### âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}", None
            data_pkg = data_processor.process_data(df, look_back, split_ratio, model_type)
            
            # 2. æ„å»ºæ¨¡å‹
            progress(0.2, desc="æ„å»ºæ¨¡å‹...")
            model = model_engine.build_model(model_type, look_back, p, d, q, auto_arima, P, D, Q, s)
            
            # 3. è®­ç»ƒæ¨¡å‹
            progress_cb = model_engine.GradioProgressCallback(progress, epochs, start_progress=0.4, end_progress=0.8)
            # ã€ä¿®æ”¹ç‚¹ã€‘ä¼ å…¥ actual_dataset_name_for_save ä½œä¸ºä¿å­˜æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
            save_path = model_engine.train_model(model_type, model, data_pkg["X_train"], data_pkg["Y_train"], epochs, batch_size, progress_cb, actual_dataset_name_for_save)
            
            # --- [æ–°å¢] ä¿å­˜å‚æ•°é…ç½® ---
            # å°†å½“å‰çš„æ‰€æœ‰å‚æ•°æ‰“åŒ…ä¿å­˜ï¼Œæ–¹ä¾¿ä¸‹æ¬¡è¯»å–
            current_params = {
                "dataset_name": dataset_name, "use_local_data": use_local_data, # ä¿å­˜æ˜¯å¦ä½¿ç”¨äº†æœ¬åœ°æ•°æ®
                "model_type": model_type,
                "epochs": epochs, "batch_size": batch_size,
                "look_back": look_back, "split_ratio": split_ratio,
                "p": p, "d": d, "q": q, "auto_arima": auto_arima,
                "P": P, "D": D, "Q": Q, "s": s
            }
            save_pipeline_config(save_path, current_params)
            
            file_size_str = utils.get_file_size_str(save_path)
            report_prefix = "### è®­ç»ƒæŠ¥å‘Š"

        # === å…¬å…±éƒ¨åˆ†ï¼šè¯„ä¼°ä¸ç»˜å›¾ ===
        # æ³¨æ„ï¼šä¸ºäº†è¯„ä¼°å’Œç»˜å›¾ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åŠ è½½æ•°æ®ã€‚
        # å“ªæ€•æ˜¯åŠ è½½æ¨¡å‹æ¨¡å¼ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦ç”¨å½“åˆä¿å­˜å‚æ•°é‡Œçš„ dataset_name ç­‰é…ç½®æ¥é‡æ–°å¤„ç†æ•°æ®ï¼Œ
        # è¿™æ ·æ‰èƒ½ä¿è¯ X_test çš„å½¢çŠ¶å’Œæ¨¡å‹çš„è¾“å…¥åŒ¹é…ã€‚
        
        progress(0.8, desc="å‡†å¤‡è¯„ä¼°æ•°æ®...")
        # ã€ä¿®æ”¹ç‚¹ã€‘å†æ¬¡åŠ è½½æ•°æ®ç”¨äºè¯„ä¼°
        try:
            # å†æ¬¡åŠ è½½æ•°æ®ç”¨äºè¯„ä¼° (ä¿è¯æ•°æ®ä¸€è‡´æ€§)
            if use_local_data:
                df = data_processor.load_raw_data("ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®", local_file_path)
            else:
                df = data_processor.load_raw_data(dataset_name)
        except Exception as e:
            return f"### âŒ è¯„ä¼°é˜¶æ®µæ•°æ®åŠ è½½å¤±è´¥: {str(e)}", None
        data_pkg = data_processor.process_data(df, look_back, split_ratio, model_type)
        
        progress(0.9, desc="æ¨¡å‹é¢„æµ‹ä¸è¯„ä¼°...")
        eval_res = model_engine.evaluate_model(
            model_type, model, 
            data_pkg["X_train"], data_pkg["X_test"], 
            data_pkg["Y_train"], data_pkg["Y_test"], 
            data_pkg["scaler"],
            future_steps if enable_future else 0  # ä¼ å…¥éœ€è¦é¢å¤–é¢„æµ‹çš„æ­¥æ•°
        )
        
        report = f"""
        {report_prefix}
        - **æ•°æ®é›†**: {actual_dataset_name_for_save}
        - **æ•°æ®æ¥æº**: {'æœ¬åœ°ä¸Šä¼ ' if use_local_data else 'ç³»ç»Ÿé¢„ç½®'}
        - **æ¨¡å‹ç±»å‹**: {model_type}
        - **MAE**: {eval_res['mae']:.4f}
        - **RMSE**: {eval_res['rmse']:.4f}
        - **é¢å¤–é¢„æµ‹**: {'å·²å¯ç”¨ (' + str(future_steps) + 'æ­¥)' if enable_future else 'æœªå¯ç”¨'}
        - **æ¨¡å‹è·¯å¾„**: `{save_path}`
        - **æ¨¡å‹æ–‡ä»¶å¤§å°**: **{file_size_str}**
        """
        
        # [ä¿®æ”¹] ä¼ å…¥ future_predict æ•°æ®åˆ°ç»˜å›¾å‡½æ•°
        fig = visualizer.create_forecast_plot(
            model_type, actual_dataset_name_for_save,
            data_pkg["full_dataset_scaled"],
            eval_res["train_predict"], 
            eval_res["test_predict"],
            eval_res.get("future_predict", None),
            look_back, data_pkg["scaler"]
        )
        
        return report, fig
    except Exception as e:
        # ã€æ–°å¢ã€‘é¡¶å±‚é”™è¯¯æ•è·ï¼Œé˜²æ­¢ Gradio å´©æºƒ
        import traceback
        traceback.print_exc() # åœ¨æ§åˆ¶å°æ‰“å°è¯¦ç»†é”™è¯¯ï¼Œæ–¹ä¾¿è°ƒè¯•
        error_msg = f"### âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯\n\n**é”™è¯¯ä¿¡æ¯**: {str(e)}\n\n*æç¤ºï¼šå¦‚æœä½¿ç”¨çš„æ˜¯å·²ä¿å­˜çš„ ARIMA/SARIMA æ¨¡å‹ï¼Œè¯·ç¡®ä¿åŠ è½½çš„æ•°æ®é›†ä¸è®­ç»ƒæ—¶ä¸€è‡´ã€‚*"
        return error_msg, None

def create_ui():
    # ä½¿ç”¨è‡ªå®šä¹‰ CSS ç¾åŒ–
    custom_css = """
    body { background-color: #0b0f19; } 
    .gradio-container { 
        font-family: 'Roboto', sans-serif; 
        margin-top: 1vh;  /* é¡¶éƒ¨ç•™ç™½ */
    }
    /* ä¿®å¤ Electron ä¸­ Markdown ç»„ä»¶ï¼ˆå¦‚æ ‡é¢˜ï¼‰å‡ºç°ä¸å¿…è¦æ»šåŠ¨æ¡çš„é—®é¢˜ */
    .prose {
        overflow: visible !important; /* å¼ºåˆ¶å†…å®¹å¯è§ï¼Œä¸è£å‰ªä¹Ÿä¸æ»šåŠ¨ */
    }
    /* å¦‚æœä¸Šè¿°æ— æ•ˆï¼Œå¯ä»¥å°è¯•æ›´æš´åŠ›çš„éšè—æ»šåŠ¨æ¡æ ·å¼ */
    .prose::-webkit-scrollbar {
        display: none; 
        width: 0 !important;
        height: 0 !important;
    }
    /* é’ˆå¯¹æ‰€æœ‰ Markdown ç±»å‹çš„å®¹å™¨ */
    .gr-markdown, .markdown-text {
        overflow: visible !important;
    }
    /* ç”šè‡³å¯ä»¥ç›´æ¥é’ˆå¯¹æ ‡é¢˜æ ‡ç­¾ */
    h1, h2, h3, h4, h5, h6 {
        overflow: visible !important;
        margin-bottom: 0.2em !important; /* æœ‰æ—¶å¢åŠ ä¸€ç‚¹ä¸‹è¾¹è·ä¹Ÿèƒ½è§£å†³è®¡ç®—è¯¯å·® */
    }

    /* å®šä¹‰ç¦ç”¨æŒ‰é’®çš„æ ·å¼ (å¯é€‰) */
    .disabled-btn { opacity: 0.5; cursor: not-allowed; }

    /* æ ¸å¿ƒï¼šé’ˆå¯¹ interactive=False çš„ç»„ä»¶åº”ç”¨æ ·å¼ */
    /* æ³¨æ„ï¼šGradio ç‰ˆæœ¬ä¸åŒç±»åå¯èƒ½ç•¥æœ‰å·®å¼‚ï¼Œè¿™é‡Œè¦†ç›–äº†å¸¸è§çš„ç¦ç”¨çŠ¶æ€ */
    input:disabled, textarea:disabled, .disabled, .gr-disabled {
        opacity: 0.4 !important;
        cursor: not-allowed !important;
    }
    /* é’ˆå¯¹æ»‘å—å’Œå®¹å™¨çš„ç¦ç”¨å±‚ */
    .pointer-events-none {
        opacity: 0.4 !important;
        pointer-events: none !important;
    }
    """
    
    with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title="æ—¶é—´åºåˆ—åˆ†æå·¥ä½œç«™") as demo:
        gr.Markdown("# ğŸ“ˆ æ—¶é—´åºåˆ—åˆ†æå·¥ä½œç«™ (Ver. 1.0.0)\n\n")

        # === çŠ¶æ€å˜é‡ (State) ===
        # ç”¨äºå­˜å‚¨æœ¬åœ°æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        local_file_path_state = gr.State(value=None)
        # ç”¨äºå­˜å‚¨ç”¨äºæ˜¾ç¤ºå’Œä¿å­˜çš„â€œå¹²å‡€â€æ•°æ®é›†åç§° (ä¾‹å¦‚: my_data)
        dataset_name_clean_state = gr.State(value="Sine Wave (æ¨¡æ‹Ÿ)")

        with gr.Column():
            gr.Markdown("### ğŸ“Š å¯è§†åŒ–çª—å£")
            plot_out = gr.Plot(label="å¯è§†åŒ–")
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ é…ç½®é¢æ¿")
                    
                    # --- æ¨¡å— A: å·²ä¿å­˜æ¨¡å‹ (æœ€é«˜ä¼˜å…ˆçº§) ---
                    with gr.Group():
                        use_saved_cb = gr.Checkbox(label="ğŸ“‚ é€‰ç”¨å·²ä¿å­˜çš„æ¨¡å‹", value=False)
                        saved_model_dd = gr.Dropdown(label="é€‰æ‹©æ¨¡å‹æ–‡ä»¶", choices=[], visible=False, interactive=True)

                    # --- æ¨¡å— B: æœ¬åœ°æ•°æ® (æ¬¡ä¼˜å…ˆçº§) ---
                    with gr.Group():
                        use_local_cb = gr.Checkbox(label="ğŸ“‚ ä¸Šä¼ æœ¬åœ°æ•°æ® (CSV)", value=False)
                        file_uploader = gr.File(
                            label="æ‹–æ‹½ä¸Šä¼  CSV æ–‡ä»¶ (éœ€ç¬¦åˆ Date-Value æ ¼å¼)", 
                            file_types=[".csv"], 
                            visible=False,
                            type="filepath"
                        )

                    # --- æ¨¡å— C: é¢„ç½®æ•°æ®é›† (é»˜è®¤) ---
                    # ç§»é™¤äº† "ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®" é€‰é¡¹
                    dataset_dd = gr.Dropdown(
                        choices=["Sine Wave (æ¨¡æ‹Ÿ)", "AirPassengers (æ¨¡æ‹Ÿ)", "AirPassengers", "Daily minimum temperatures in Melbourne", "Sunspots", "Mauna Loa CO2 Weekly", "Arctic Oscillation Dataset"], 
                        value="Sine Wave (æ¨¡æ‹Ÿ)", 
                        label="é¢„ç½®æ•°æ®é›†"
                    )

                    model_dd = gr.Dropdown(choices=["LSTM", "MLP", "ARIMA", "SARIMA", "Exponential-Smoothing"], value="LSTM", label="æ¨¡å‹")

                    with gr.Group():
                        enable_future_cb = gr.Checkbox(label="ğŸ”® æ˜¯å¦é¢å¤–é¢„æµ‹æ•°æ®", value=False, interactive=True)
                        future_steps_sl = gr.Slider(minimum=1, maximum=100, value=12, step=1, label="é¢å¤–é¢„æµ‹æ­¥æ•°", visible=False, interactive=True)

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ”§ å‚æ•°è®¾ç½®")
                    with gr.Column(visible=True) as dl_params_group:
                        epochs_sl = gr.Slider(minimum=1, maximum=100, value=20, step=1, label="è®­ç»ƒè½®æ¬¡ (Epochs)")
                        batch_sl = gr.Slider(minimum=1, maximum=64, value=16, step=1, label="Batch Size")

                    with gr.Column(visible=False) as arima_params_group:
                        auto_arima = gr.Checkbox(value=True, label="æ˜¯å¦ä½¿ç”¨(S)ARIMAçš„è‡ªåŠ¨å‚æ•°æ¨å¯¼")
                        with gr.Column(visible=False) as manual_params_container:
                            gr.Markdown("#### ARIMA åŸºç¡€å‚æ•°")
                            p_sl = gr.Slider(minimum=0, maximum=10, value=2, step=1, label="p")
                            d_sl = gr.Slider(minimum=0, maximum=10, value=1, step=1, label="d")
                            q_sl = gr.Slider(minimum=0, maximum=10, value=2, step=1, label="q")
                            with gr.Column(visible=False) as sarima_pdq_group:
                                gr.Markdown("#### SARIMA å­£èŠ‚æ€§å‚æ•°")
                                P_sl = gr.Slider(minimum=0, maximum=10, value=2, step=1, label="P")
                                D_sl = gr.Slider(minimum=0, maximum=10, value=1, step=1, label="D")
                                Q_sl = gr.Slider(minimum=0, maximum=10, value=2, step=1, label="Q")
                        with gr.Column(visible=False) as sarima_s_group:
                            s_sl = gr.Slider(minimum=0, maximum=100, value=12, step=1, label="s")

                    look_back_sl = gr.Slider(minimum=1, maximum=100, value=3, step=1, label="æ—¶é—´çª—å£ (Look Back)")
                    split_sl = gr.Slider(minimum=0.5, maximum=0.9, value=0.7, step=0.05, label="è®­ç»ƒé›†å æ¯”")
                    
                # å®šä¹‰å‚æ•°åˆ—è¡¨ï¼Œç”¨äºæ‰¹é‡ç¦ç”¨/å¯ç”¨
                # è¿™äº›å‚æ•°åœ¨åŠ è½½æ¨¡å‹æ—¶ä¼šè¢«ç¦ç”¨ï¼ˆå› ä¸ºå®ƒä»¬æ˜¯æ¨¡å‹ç»“æ„çš„ä¸€éƒ¨åˆ†ï¼‰
                train_params_locked = [
                    model_dd, epochs_sl, batch_sl, look_back_sl, split_sl,
                    auto_arima, p_sl, d_sl, q_sl, P_sl, D_sl, Q_sl, s_sl
                ]
                # è¿™äº›å‚æ•°åœ¨ä»»ä½•æ—¶å€™éƒ½åº”è¯¥å…è®¸ç”¨æˆ·ä¿®æ”¹
                predict_params = [enable_future_cb, future_steps_sl]

                with gr.Column(scale=1):
                    gr.Markdown("### â–¶ï¸ è¿è¡ŒæŒ‰é”®")
                    # æŒ‰é’®åŒº
                    with gr.Row():
                        btn_run = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒä¸è¯„ä¼°", variant="primary")
                        btn_stop = gr.Button("â¹ï¸ ç»ˆæ­¢å½“å‰ä»»åŠ¡", variant="secondary")

                    btn_clear = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå·²ä¿å­˜æ¨¡å‹", variant="stop") 
                    metrics_out = gr.Markdown("### ç­‰å¾…æŒ‡ä»¤...")


        # =========================================
        # 4. äº¤äº’é€»è¾‘ (Callbacks)
        # =========================================

        # --- A. ç•Œé¢å¯è§æ€§æ§åˆ¶å‡½æ•° ---

        def update_lookback_visibility(model_type):
            return gr.update(visible=True) if model_type not in ["ARIMA", "SARIMA", "Exponential-Smoothing"] else gr.update(visible=False)

        def update_dl_params_visibility(model_type):
            return gr.update(visible=(model_type in ["LSTM", "MLP"]))

        def update_arima_container_visibility(model_type):
            return gr.update(visible=(model_type in ["ARIMA", "SARIMA"]))

        def update_sarima_sub_visibility(model_type, auto_mode):
            if model_type == "SARIMA":
                return gr.update(visible=not auto_mode), gr.update(visible=True)
            return gr.update(visible=False), gr.update(visible=False)

        def update_manual_params_visibility(auto_mode):
            return gr.update(visible=not auto_mode)
        
        # --- B. æ•°æ®æºåˆ‡æ¢é€»è¾‘ (æ ¸å¿ƒä¿®æ”¹) ---

        def on_use_saved_change(use_saved):
            """
            ä¿®æ”¹åé€»è¾‘ï¼š
            1. ç¦ç”¨/å¯ç”¨ 'train_params_locked' (æ¨¡å‹ç»“æ„å‚æ•°)ã€‚
            2. å§‹ç»ˆä¿æŒ 'predict_params' (é¢„æµ‹æ­¥æ•°) ä¸º Interactive=Trueã€‚
            """
            # 1. è®­ç»ƒå‚æ•°ï¼šæ ¹æ®æ˜¯å¦ä½¿ç”¨ä¿å­˜æ¨¡å‹æ¥é”å®š
            params_interactive = not use_saved
            lock_updates = [gr.update(interactive=params_interactive) for _ in train_params_locked]
            
            # 2. é¢„æµ‹å‚æ•°ï¼šå§‹ç»ˆå…è®¸ä¿®æ”¹
            # æ³¨æ„ï¼švisibility ä¾ç„¶ç”± enable_future_cb è‡ªèº«çš„é€»è¾‘æ§åˆ¶ï¼Œè¿™é‡Œåªç®¡ interactive
            predict_updates = [
                gr.update(interactive=True), # enable_future_cb
                gr.update(interactive=True)  # future_steps_sl
            ]
            
            # 3. å…¶ä»–UIç»„ä»¶
            file_list = get_saved_model_list() if use_saved else []
            if not file_list:
                saved_model_dd_update = gr.update(visible=use_saved, choices=[], value=None)
            else:
                # ä¿æŒå½“å‰å€¼ï¼ˆå¦‚æœåœ¨åˆ—è¡¨ä¸­ï¼‰ï¼Œæˆ–è€…è®¾ä¸º None
                saved_model_dd_update = gr.update(visible=use_saved, choices=file_list)
            
            btn_text = "ğŸš« è¯·å…ˆé€‰æ‹©æ¨¡å‹æ–‡ä»¶" if use_saved else "ğŸš€ å¼€å§‹è®­ç»ƒä¸è¯„ä¼°"
            btn_update = gr.update(interactive=not use_saved, value=btn_text, variant="secondary" if use_saved else "primary")
            
            # è¿”å›åˆ—è¡¨é¡ºåºå¿…é¡»ä¸ outputs å®šä¹‰ä¸€è‡´ï¼šLock Params + Predict Params + [Saved DD, Btn, Local CB, Dataset DD]
            return lock_updates + predict_updates + [
                saved_model_dd_update, 
                btn_update, 
                gr.update(interactive=True),
                gr.update(interactive=True)
            ]

        def on_use_local_change(use_local):
            """
            ä¿®æ”¹åé€»è¾‘ï¼š
            å®Œå…¨è§£è€¦ã€‚åªçœ‹ use_local çš„å€¼ï¼Œä¸çœ‹ use_savedã€‚
            - å‹¾é€‰æœ¬åœ°: æ˜¾ç¤ºä¸Šä¼ æ¡†ï¼Œç¦ç”¨é¢„ç½®ä¸‹æ‹‰
            - å–æ¶ˆæœ¬åœ°: éšè—ä¸Šä¼ æ¡†ï¼Œå¯ç”¨é¢„ç½®ä¸‹æ‹‰
            """
            if use_local:
                return gr.update(visible=True), gr.update(interactive=False)
            else:
                return gr.update(visible=False), gr.update(interactive=True)

        # --- C. æ•°æ®åŠ è½½ä¸é¢„è§ˆ ---

        def update_preview_by_preset(dataset_name, use_local):
            """é¢„ç½®æ•°æ®é›†æ”¹å˜ -> æ›´æ–°é¢„è§ˆ (ä»…å½“æœªä½¿ç”¨æœ¬åœ°æ•°æ®æ—¶)"""
            if use_local: 
                return None # ä¸æ›´æ–°ï¼Œä¿æŒå½“å‰æœ¬åœ°æ•°æ®çš„å›¾
            
            try:
                df = data_processor.load_raw_data(dataset_name)
                fig = visualizer.create_data_preview_plot(dataset_name, df)
                return fig
            except:
                return None

        def on_file_upload(file_path):
            """æ–‡ä»¶ä¸Šä¼ å®Œæ¯• -> æ ¡éªŒå¹¶æ›´æ–°é¢„è§ˆ"""
            if not file_path:
                return gr.update(), None, None, None
            
            file_name_clean = os.path.splitext(os.path.basename(file_path))[0]
            try:
                df = data_processor.validate_and_load_local(file_path)
                fig = visualizer.create_data_preview_plot(f"æœ¬åœ°æ•°æ®: {file_name_clean}", df)
                msg = f"### âœ… æˆåŠŸåŠ è½½æœ¬åœ°æ•°æ®: {os.path.basename(file_path)}\næ ·æœ¬æ•°: {len(df)}"
                return msg, fig, file_name_clean, file_path
            except ValueError as e:
                return f"### âŒ æ ¼å¼é”™è¯¯: {str(e)}", None, None, None

        # --- D. çº¦æŸä¸å‚æ•°å›å¡« ---

        def update_lookback_constraints(dataset_name, use_local, local_path, split_ratio, current_lookback):
            """æ ¹æ®å½“å‰é€‰ä¸­çš„æ•°æ®æºï¼ˆé¢„ç½®æˆ–æœ¬åœ°ï¼‰è®¡ç®—çº¦æŸ"""
            try:
                if use_local:
                    if not local_path: return gr.update(), "### âš ï¸ è¯·å…ˆä¸Šä¼  CSV"
                    df = data_processor.load_raw_data("ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®", local_path)
                else:
                    df = data_processor.load_raw_data(dataset_name)
                
                total_len = len(df)
                train_size = int(total_len * split_ratio)
                new_max = max(1, train_size - 2)
                new_value = min(current_lookback, new_max)
                
                return gr.update(maximum=new_max, value=new_value), f"å½“å‰æ•°æ®æ€»é•¿: {total_len}, è®­ç»ƒé›†: {train_size}"
            except:
                return gr.update(), ""

        def on_saved_model_select(filename):
            """é€‰æ‹©æ¨¡å‹ -> å›å¡«å‚æ•° (é¢„æµ‹å‚æ•°ä¸å—å½±å“ï¼Œä¿æŒå½“å‰UIçŠ¶æ€)"""
            if not filename: return [gr.skip()] * 15
            params = load_pipeline_config(filename)
            if not params: return [gr.update()] * 14 + [gr.update(interactive=True, value="âš ï¸ å…ƒæ•°æ®ä¸¢å¤±")]
            
            return [
                gr.update(value=params.get("dataset_name"), interactive=True), 
                gr.update(value=params.get("model_type"), interactive=False),   
                params.get("epochs", 20), params.get("batch_size", 16), params.get("look_back", 3), params.get("split_ratio", 0.7),          
                params.get("p", 2), params.get("d", 1), params.get("q", 2),
                params.get("auto_arima", True),          
                params.get("P", 2), params.get("D", 1), params.get("Q", 2), params.get("s", 12),
                gr.update(interactive=True, value="ğŸš€ åŠ è½½æ¨¡å‹å¹¶è¯„ä¼°", variant="primary") 
            ]

        # =========================================
        # 5. äº‹ä»¶ç»‘å®š (Event Wiring)
        # =========================================
        
        # 1. â€œå·²ä¿å­˜æ¨¡å‹â€ å‹¾é€‰é€»è¾‘
        use_saved_cb.change(
            fn=on_use_saved_change,
            inputs=use_saved_cb,
            # Outputs å¿…é¡»åŒ…å«æ‰€æœ‰è¢«ä¿®æ”¹çš„ç»„ä»¶
            outputs=train_params_locked + predict_params + [saved_model_dd, btn_run, use_local_cb, dataset_dd]
        )
        
        # 2. â€œæœ¬åœ°æ•°æ®â€ å‹¾é€‰ (å»æ‰äº† use_saved_cb è¾“å…¥)
        use_local_cb.change(
            fn=on_use_local_change,
            inputs=[use_local_cb], # åªéœ€è¦è¿™ä¸€ä¸ªè¾“å…¥
            outputs=[file_uploader, dataset_dd]
        )
        
        # 3. æ–‡ä»¶ä¸Šä¼ é€»è¾‘
        upload_event = file_uploader.change(
            fn=on_file_upload,
            inputs=file_uploader,
            outputs=[metrics_out, plot_out, dataset_name_clean_state, local_file_path_state]
        )
        
        # 4. é¢„ç½®æ•°æ®é›†åˆ‡æ¢é€»è¾‘ (ä»…æ›´æ–°å›¾)
        dataset_dd.change(
            fn=update_preview_by_preset,
            inputs=[dataset_dd, use_local_cb],
            outputs=plot_out
        )

        # 5. å‚æ•°å¯è§æ€§è”åŠ¨ (Model -> Params)
        model_dd.change(fn=update_dl_params_visibility, inputs=model_dd, outputs=dl_params_group)
        model_dd.change(fn=update_arima_container_visibility, inputs=model_dd, outputs=arima_params_group)
        model_dd.change(fn=update_sarima_sub_visibility, inputs=[model_dd, auto_arima], outputs=[sarima_pdq_group, sarima_s_group])
        model_dd.change(fn=update_lookback_visibility, inputs=model_dd, outputs=look_back_sl)
        auto_arima.change(fn=update_manual_params_visibility, inputs=auto_arima, outputs=manual_params_container)
        auto_arima.change(fn=update_sarima_sub_visibility, inputs=[model_dd, auto_arima], outputs=[sarima_pdq_group, sarima_s_group])
        enable_future_cb.change(fn=lambda x: gr.update(visible=x), inputs=enable_future_cb, outputs=future_steps_sl)

        # 6. è‡ªåŠ¨çº¦æŸ Lookback (æ‰€æœ‰å¯èƒ½æ”¹å˜æ•°æ®é•¿åº¦çš„æ“ä½œéƒ½è¦è§¦å‘)
        constraint_inputs = [dataset_dd, use_local_cb, local_file_path_state, split_sl, look_back_sl]
        
        # ç»‘å®šåˆ° upload ç»“æŸ
        upload_event.then(fn=update_lookback_constraints, inputs=constraint_inputs, outputs=[look_back_sl, metrics_out])
        # ç»‘å®šåˆ° dataset åˆ‡æ¢
        dataset_dd.change(fn=update_lookback_constraints, inputs=constraint_inputs, outputs=[look_back_sl, metrics_out])
        # ç»‘å®šåˆ° split å˜åŒ–
        split_sl.change(fn=update_lookback_constraints, inputs=constraint_inputs, outputs=[look_back_sl, metrics_out])

        # 7. é€‰æ‹©å·²ä¿å­˜æ¨¡å‹æ–‡ä»¶
        saved_model_dd.change(
            fn=on_saved_model_select,
            inputs=saved_model_dd,
            outputs=[dataset_dd, model_dd, epochs_sl, batch_sl, look_back_sl, split_sl, p_sl, d_sl, q_sl, auto_arima, P_sl, D_sl, Q_sl, s_sl, btn_run]
        )

        # 8. è¿è¡ŒæŒ‰é’®
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥ use_local_cb çš„å€¼
        run_event = btn_run.click(
            fn=run_pipeline,
            inputs=[
                dataset_dd, use_local_cb, local_file_path_state, dataset_name_clean_state, # <--- Updated inputs
                model_dd, epochs_sl, batch_sl, look_back_sl, split_sl, 
                p_sl, d_sl, q_sl, auto_arima, P_sl, D_sl, Q_sl, s_sl,
                use_saved_cb, saved_model_dd,
                enable_future_cb, future_steps_sl
            ],
            outputs=[metrics_out, plot_out]
        )
        
        btn_stop.click(fn=lambda: ("### âš ï¸ ä»»åŠ¡å·²ç»ˆæ­¢", None), outputs=[metrics_out, plot_out], cancels=[run_event])
        
        btn_clear.click(
            fn=lambda: (clean_model_history(), False, gr.update(visible=False, choices=[], value=None), gr.update(interactive=True)), 
            outputs=[metrics_out, use_saved_cb, saved_model_dd, btn_run]
        )

        # åˆå§‹åŒ–åŠ è½½
        demo.load(fn=update_preview_by_preset, inputs=[dataset_dd, use_local_cb], outputs=plot_out)

    return demo

def register_shutdown(app: FastAPI):
    @app.get("/shutdown")
    def shutdown():
        print("Service shutting down...")
        utils.shutdown_server()
        return {"status": "ok"}