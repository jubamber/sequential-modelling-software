
# æ•°æ®å±‚ï¼šåªè´Ÿè´£æ•°æ®çš„è¯»å–ã€æ¸…æ´—ã€è½¬æ¢ã€‚ä¸æ¶‰åŠä»»ä½•æ¨¡å‹æˆ–UIä»£ç 

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import os
import utils

def validate_and_load_local(file_path):
    """
    è§£ææœ¬åœ°ä¸Šä¼ çš„ CSV/TXT æ–‡ä»¶ã€‚
    æ”¯æŒï¼š
    1. è‡ªåŠ¨è¯†åˆ«é€—å·ã€ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ç­‰åˆ†éš”ç¬¦ã€‚
    2. è‡ªåŠ¨åˆ¤æ–­ç¬¬ä¸€è¡Œæ˜¯å¦ä¸ºè¡¨å¤´ï¼ˆHeaderï¼‰ã€‚
    3. ç¬¬ä¸€åˆ—ä½œä¸ºç´¢å¼•ï¼šä¼˜å…ˆå°è¯•è½¬ä¸ºæ—¶é—´ï¼Œè‹¥éæ—¶é—´æ ¼å¼åˆ™æ‰“å°è­¦å‘Šå¹¶ä¿ç•™åŸæ ·ã€‚
    4. ç¬¬äºŒåˆ—ä½œä¸ºæ•°å€¼ï¼šéæ•°å€¼è¡Œå°†è¢«å‰”é™¤ã€‚
    """
    try:
        # 1. è¯»å–æ–‡ä»¶
        # sep=None + engine='python' èƒ½å¤Ÿè‡ªåŠ¨å—…æ¢åˆ†éš”ç¬¦ï¼ˆé€—å·ã€ç©ºæ ¼ç­‰ï¼‰
        # header=None: å…ˆæŠŠæ‰€æœ‰å†…å®¹è¯»è¿›æ¥ï¼Œåç»­æ‰‹åŠ¨åˆ¤æ–­å“ªä¸€è¡Œæ˜¯æ•°æ®
        df_raw = pd.read_csv(file_path, sep=None, engine='python', header=None)
    except Exception as e:
        raise ValueError(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æŸåã€‚\nåº•å±‚é”™è¯¯: {e}")

    # 2. åˆ—æ•°æ ¡éªŒ
    if df_raw.shape[1] < 2:
        raise ValueError(f"è¯†åˆ«åˆ°çš„åˆ—æ•°ä¸è¶³ï¼ˆå½“å‰åˆ—æ•°: {df_raw.shape[1]}ï¼‰ï¼Œè¯·ç¡®ä¿æ•°æ®è‡³å°‘åŒ…å«ä¸¤åˆ—ï¼ˆç´¢å¼•å’Œæ•°å€¼ï¼‰ï¼Œå¹¶ä½¿ç”¨ä¸€è‡´çš„åˆ†éš”ç¬¦ã€‚")

    # æˆªå–å‰ä¸¤åˆ—ï¼šå‡è®¾ç¬¬ä¸€åˆ—æ˜¯ç´¢å¼•ï¼Œç¬¬äºŒåˆ—æ˜¯æ•°å€¼
    df = df_raw.iloc[:, :2].copy()
    
    # 3. æ™ºèƒ½åˆ¤æ–­ç¬¬ä¸€è¡Œæ˜¯å¦ä¸ºè¡¨å¤´
    # é€»è¾‘ï¼šæ£€æŸ¥ç¬¬äºŒåˆ—ï¼ˆæ•°å€¼åˆ—ï¼‰çš„ç¬¬ä¸€è¡Œå†…å®¹
    first_val_raw = df.iloc[0, 1]
    
    # å°è¯•å°†ç¬¬ä¸€è¡Œç¬¬äºŒåˆ—è½¬ä¸ºæ•°å­—
    is_header = False
    try:
        pd.to_numeric(first_val_raw, float_precision='high')
    except (ValueError, TypeError):
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼ˆä¾‹å¦‚å®ƒæ˜¯å­—ç¬¦ä¸² "Value"ï¼‰ï¼Œåˆ™è®¤ä¸ºæ˜¯è¡¨å¤´
        is_header = True
    
    # å¦‚æœç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´ï¼Œåˆ å»ç¬¬ä¸€è¡Œ
    if is_header:
        df = df.iloc[1:].copy()

    # é‡æ–°è®¾ç½®åˆ—åï¼Œæ–¹ä¾¿åç»­å¤„ç†
    df.columns = ['_raw_index', 'Value']

    # 4. æ•°æ®æ¸…æ´—ï¼šå¤„ç†æ•°å€¼åˆ—
    # å¼ºåˆ¶è½¬ä¸ºæ•°å€¼ï¼Œæ— æ³•è½¬æ¢çš„å˜ä¸º NaN
    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')
    
    # åˆ é™¤æ•°å€¼ä¸ºç©ºçš„è¡Œ
    df.dropna(subset=['Value'], inplace=True)
    
    if df.empty:
        raise ValueError("æ¸…æ´—åæœ‰æ•ˆæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°å€¼ã€‚")

    # 5. å¤„ç†ç´¢å¼•åˆ— (Date/Index)
    raw_index_col = df['_raw_index']
    
    try:
        # å°è¯•è½¬æ¢ä¸º datetime
        # errors='coerce' ä¼šå°†æ— æ³•è½¬æ¢çš„å˜ä¸º NaT
        datetime_index = pd.to_datetime(raw_index_col, errors='coerce')
        
        # æ ¡éªŒè½¬æ¢æˆåŠŸç‡ï¼šå¦‚æœè¶…è¿‡ä¸€åŠçš„æ•°æ®éƒ½æ— æ³•è½¬ä¸ºæ—¶é—´ï¼Œè¯´æ˜è¿™å¯èƒ½ä¸æ˜¯æ—¶é—´åˆ—
        if datetime_index.notna().sum() < 0.5 * len(df):
            raise ValueError("Time conversion failed mostly")
            
        df.index = datetime_index
        df.index.name = 'Date'
        
        # å‰”é™¤æ—¶é—´è½¬æ¢å¤±è´¥çš„è¡Œï¼ˆå¯é€‰ï¼Œä¿è¯æ—¶é—´åºåˆ—çš„çº¯å‡€æ€§ï¼‰
        # df = df[df.index.notna()] 
        
    except Exception:
        # ã€ä¿®æ”¹ç‚¹ã€‘ä¸å¼ºåˆ¶æŠ¥é”™ï¼Œè€Œæ˜¯æ‰“å°è­¦å‘Šå¹¶ä½¿ç”¨åŸå§‹ç´¢å¼•
        print("è­¦å‘Šï¼šç¬¬ä¸€åˆ—æ— æ³•è¯†åˆ«ä¸ºæ ‡å‡†æ—¶é—´æ ¼å¼(Date)ï¼Œå·²ä¿ç•™åŸå§‹ç´¢å¼•ã€‚")
        df.index = raw_index_col
        df.index.name = 'Index'

    # ç§»é™¤ä¸´æ—¶åˆ—
    df.drop(columns=['_raw_index'], inplace=True)

    # 6. æœ€ç»ˆæ’åºä¸è¾“å‡º
    try:
        df.sort_index(inplace=True)
    except TypeError:
        # å¦‚æœç´¢å¼•æ˜¯æ··åˆç±»å‹ï¼ˆå­—ç¬¦ä¸²+æ•°å­—ï¼‰ï¼Œæ’åºå¯èƒ½ä¼šå¤±è´¥ï¼Œæ­¤æ—¶å¿½ç•¥æ’åº
        pass

    return df

def load_raw_data(dataset_name, local_file_path=None):
    """ç”Ÿæˆæˆ–åŠ è½½åŸå§‹æ•°æ® DataFrame"""
    if dataset_name == "ğŸ“‚ åŠ è½½æœ¬åœ°æ•°æ®":
        if not local_file_path or not os.path.exists(local_file_path):
            raise ValueError("æœªæ‰¾åˆ°ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ã€‚")
        return validate_and_load_local(local_file_path)
    
    elif dataset_name == "Sine Wave (æ¨¡æ‹Ÿ)":
        x = np.linspace(0, 50, 1000)
        y = np.sin(x) + np.random.normal(0, 0.1, 1000) # åŠ ç‚¹å™ªå£°
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šå°† x è®¾ä¸ºç´¢å¼•ï¼Œæ˜ç¡®åˆ—åä¸º Value
        # è¿™æ · df.iloc[:, 0] å–åˆ°çš„å°±æ˜¯ y (Value)ï¼Œè€Œä¸æ˜¯ x
        df = pd.DataFrame(data=y, index=x, columns=['Value'])
        df.index.name = 'Date' # ä¿æŒç´¢å¼•åä¸€è‡´
    elif dataset_name == "AirPassengers (æ¨¡æ‹Ÿ)":
        # æ¨¡æ‹Ÿå¢é•¿è¶‹åŠ¿ + å­£èŠ‚æ€§
        x = np.linspace(0, 10, 1000)
        y = x * 0.5 + np.sin(x * 5) + np.random.normal(0, 0.2, 1000)
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ï¼šåŒä¸Š
        df = pd.DataFrame(data=y, index=x, columns=['Value'])
        df.index.name = 'Date'
    elif dataset_name == "AirPassengers":
        df = pd.read_csv(
            utils.get_resource_path("datasets/AirPassengers.csv"),
            header = 0, 
            parse_dates = [0], 
            names = ['Date', 'Value'], 
            index_col = 0
        )
    elif dataset_name == "Daily minimum temperatures in Melbourne":
        df = pd.read_csv(
            utils.get_resource_path("datasets/daily-minimum-temperatures-in-me.csv"), 
            on_bad_lines='skip', # é‡åˆ°æ ¼å¼é”™è¯¯çš„è¡Œç›´æ¥è·³è¿‡
            #skipfooter=1,       # å¿½ç•¥æœ€åä¸€è¡Œ
            header = 0, 
            parse_dates = [0], 
            names = ['Date', 'Value'], 
            index_col = 0
        )
    elif dataset_name == "Sunspots":
        df = pd.read_csv(
            utils.get_resource_path("datasets/sunspots.csv"), 
            header=0,                  # æŒ‡å®šç¬¬0è¡Œä¸ºè¡¨å¤´
            usecols=[1, 2],            # å…³é”®ç‚¹ï¼šåªè¯»å–ç¬¬2åˆ—(Date)å’Œç¬¬3åˆ—(Value)ï¼Œå¿½ç•¥ç¬¬1åˆ—çš„åºåˆ—å·
            parse_dates=[0],           # è§£æè¯»å–åçš„ç¬¬1åˆ—ï¼ˆå³Dateï¼‰ä¸ºæ—¶é—´æ ¼å¼
            names=['Date', 'Value'],   # å°†è¯»å–çš„ä¸¤åˆ—é‡å‘½åä¸ºæ ‡å‡†æ ¼å¼
            index_col=0                # å°†è¯»å–åçš„ç¬¬1åˆ—ï¼ˆå³Dateï¼‰è®¾ä¸ºç´¢å¼•
        )
    elif dataset_name == "Mauna Loa CO2 Weekly":
        df = pd.read_csv(
            utils.get_resource_path("datasets/co2_weekly_16Aug2025.txt"), # è¯·æ›¿æ¢ä¸ºä½ å®é™…çš„æ–‡ä»¶å
            sep=r'\s+',                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»»æ„é•¿åº¦çš„ç©ºç™½å­—ç¬¦ä½œä¸ºåˆ†éš”ç¬¦
            comment='#',               # å¿½ç•¥ä»¥ # å¼€å¤´çš„æ³¨é‡Šè¡Œ
            header=None,               # ä¸ä½¿ç”¨æ–‡ä»¶è‡ªå¸¦çš„è¡¨å¤´ï¼ˆå› ä¸ºæ ¼å¼ä¸æ ‡å‡†ï¼‰
            skiprows=2,                # å¿½ç•¥æ³¨é‡Šè¡Œä¹‹åçš„ä¸¤è¡Œæ–‡å­—æ ‡é¢˜ ("Start of week...", "(yr, mon...)")
                                       # æ³¨æ„ï¼šå¦‚æœè¯»å–æŠ¥é”™ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æ–‡ä»¶è°ƒæ•´æ­¤æ•°å€¼ï¼Œæˆ–ç»“åˆ on_bad_lines='skip'
            names=['Year', 'Month', 'Day', 'Decimal', 'Value', 'Days', '1yr', '10yr', 'Since1800'], # æ‰‹åŠ¨å®šä¹‰æ‰€æœ‰åˆ—å
            usecols=[0, 1, 2, 4],      # åªè¯»å–ç¬¬0,1,2åˆ—(å¹´æœˆæ—¥)å’Œç¬¬4åˆ—(CO2 ppmå€¼)
            parse_dates={'Date': [0, 1, 2]}, # å°†è¯»å–çš„å‰ä¸‰åˆ—åˆå¹¶è§£æåä¸º 'Date' çš„æ—¶é—´åˆ—
            index_col='Date',          # å°†è§£æå‡ºçš„ 'Date' åˆ—è®¾ä¸ºç´¢å¼•
            na_values=[-999.99]        # å°† -999.99 è¯†åˆ«ä¸º NaN (ç©ºå€¼)
        )
        # 2. ç¡®ä¿ç´¢å¼•æ’åºï¼ˆæ’å€¼å‰å¿…é¡»ä¿è¯æ—¶é—´æ˜¯é¡ºåºçš„ï¼‰
        df = df.sort_index()

        # 3. å¤„ç†æ—¶é—´è½´æ–­è£‚ï¼ˆå¯é€‰ä½†æ¨èï¼‰
        # å¦‚æœæ•°æ®ä¸­ä¸ä»…æœ‰ NaNï¼Œè¿˜å®Œå…¨ç¼ºå¤±äº†æŸäº›å‘¨çš„è¡Œï¼Œéœ€è¦å…ˆé‡é‡‡æ ·ç”Ÿæˆè¿ç»­çš„æ—¶é—´è½´
        # 'W' ä»£è¡¨æŒ‰å‘¨é‡é‡‡æ ·ï¼Œæ ¹æ®å®é™…æ•°æ®ä¹Ÿå¯ä»¥ç”¨ 'W-SAT' (å‘¨å…­) ç­‰
        df = df.resample('W').asfreq() 

        # 4. ä½¿ç”¨æ’å€¼å¡«å……ä¸­é—´ç¼ºå¤±å€¼
        # method='time' ä¼šæ ¹æ®æ—¶é—´ç´¢å¼•çš„è·ç¦»è¿›è¡Œæ’å€¼ï¼Œæ¯” 'linear' æ›´é€‚åˆæ—¶é—´åºåˆ—
        df['Value'] = df['Value'].interpolate(method='time')

        # 5. å¤„ç†å¼€å¤´æˆ–ç»“å°¾å¯èƒ½æ®‹ç•™çš„ NaN (å¦‚æœå¼€å¤´å°±æ˜¯ç¼ºå¤±å€¼ï¼Œæ’å€¼æ— æ³•å¡«å……)
        # ä½¿ç”¨ bfill (å‘åå¡«å……) å¤„ç†å¼€å¤´ï¼Œffill (å‘å‰å¡«å……) å¤„ç†ç»“å°¾
        df['Value'] = df['Value'].bfill().ffill()

        # 6. å†æ¬¡ç¡®è®¤æ˜¯å¦è¿˜æœ‰ç©ºå€¼ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if df['Value'].isnull().any():
            print("è­¦å‘Šï¼šæ•°æ®ä¸­ä»å­˜åœ¨æ— æ³•å¡«å……çš„ç©ºå€¼")
    elif dataset_name == "Arctic Oscillation Dataset":
        df = pd.read_csv(
        utils.get_resource_path("datasets/monthly.ao.index.b50.current.ascii"),
        sep='\s+',                 # å…³é”®ç‚¹1ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»»æ„æ•°é‡çš„ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦
        header=None,               # å…³é”®ç‚¹2ï¼šåŸæ•°æ®æ²¡æœ‰æ ‡é¢˜è¡Œï¼Œæ‰€ä»¥è®¾ä¸ºNone
        names=['Year', 'Month', 'Value'], # æ‰‹åŠ¨æŒ‡å®šåˆ—å
        parse_dates={'Date': [0, 1]},     # å…³é”®ç‚¹3ï¼šå°†ç¬¬0åˆ—(Year)å’Œç¬¬1åˆ—(Month)åˆå¹¶è§£æåä¸º'Date'
        index_col='Date'           # å°†åˆå¹¶åçš„æ—¥æœŸåˆ—è®¾ä¸ºç´¢å¼•
    )
    else:
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºè¯»å–æœ¬åœ° CSV
        df = pd.DataFrame({'Date': [], 'Value': []})
    return df


def _create_dataset(dataset, look_back=1):
    """è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæ—¶åºçª—å£"""
    # look_back = ç”¨è¿‡å»å¤šå°‘ä¸ªæ—¶é—´æ­¥æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒæ˜¯æ—¶é—´åºåˆ—å»ºæ¨¡çš„çª—å£å¤§å°ï¼Œlook_backè¶Šå¤§ï¼Œæ¨¡å‹å¯ä»¥çœ‹åˆ°æ›´å¤šå†å²ä¿¡æ¯ï¼Œä½†ç‰¹å¾ç»´åº¦ä¹Ÿä¼šå¢åŠ 
    X, Y = [], []
    for i in range(len(dataset)-look_back): 
        # TODO é€‰å–å¤šåˆ—ç‰¹å¾
        a = dataset[i:(i+look_back), 0] # åªå–ç¬¬ä¸€åˆ—çš„ï¼Œå¦‚æœfeatureså¤šç»´çš„è¯éœ€è¦ä¿®æ”¹
        X.append(a)
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

def process_data(df, look_back, split_ratio, model_type):
    """
    æ ¹æ®æ¨¡å‹ç±»å‹å†³å®šæ•°æ®å¤„ç†æ–¹å¼ï¼š
    - LSTM/MLP: ä½¿ç”¨ look_back æ„å»ºæ»‘åŠ¨çª—å£ X, Y
    - ARIMA/SARIMA/ES: ç›´æ¥è¿”å›å®Œæ•´åºåˆ—ï¼Œä¸æ„å»ºçª—å£
    """
    # è¯»å–æ•°æ®
    data = df['Value'].values.reshape(-1, 1)
    
    # å½’ä¸€åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    dataset = scaler.fit_transform(data)
    
    # è®¡ç®—åˆ‡åˆ†ç‚¹
    train_size = int(len(dataset) * split_ratio)
    
    # --- åˆ†æ”¯é€»è¾‘å¼€å§‹ ---
    
    # A. ç»Ÿè®¡å­¦æ¨¡å‹ (ARIMA, SARIMA, ETS) -> ä¸ä½¿ç”¨ Lookback
    if model_type in ["ARIMA", "SARIMA", "Exponential-Smoothing"]:
        # è®­ç»ƒé›†ï¼šç›´æ¥æˆªå–
        train = dataset[0:train_size, :]
        # æµ‹è¯•é›†ï¼šç´§æ¥è®­ç»ƒé›†ä¹‹å (ä¸éœ€è¦é‡å ï¼Œå› ä¸ºä¸éœ€è¦çª—å£)
        test = dataset[train_size:len(dataset), :]
        
        # å¯¹äº ARIMA ç±»æ¨¡å‹ï¼Œsktime åªéœ€è¦ Y (åºåˆ—æœ¬èº«)ï¼ŒX (ç‰¹å¾) å¯ä»¥ä¸ºç©º
        # ä¸ºäº†ä¿æŒ dict ç»“æ„ä¸€è‡´ï¼Œæˆ‘ä»¬å°†åºåˆ—èµ‹å€¼ç»™ Y_train/Y_test
        # X_train/X_test è®¾ä¸º None æˆ–ç©ºå ä½ç¬¦
        
        return {
            "X_train": None, 
            "Y_train": train, # (n_train, 1)
            "X_test": None, 
            "Y_test": test,   # (n_test, 1)
            "scaler": scaler,
            "full_dataset_scaled": dataset
        }
        
    # B. æ·±åº¦å­¦ä¹ æ¨¡å‹ (LSTM, MLP) -> ä½¿ç”¨ Lookback
    else:
        # è®­ç»ƒé›†
        train = dataset[0:train_size, :]
        
        # æµ‹è¯•é›†ï¼šä¸ºäº†ä¸ä¸¢å¤±å¼€å¤´çš„ look_back æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦å›æº¯
        test = dataset[train_size - look_back : len(dataset), :]
        
        # åˆ›å»ºæ—¶åºæ•°æ®
        X_train, Y_train = _create_dataset(train, look_back)
        X_test, Y_test = _create_dataset(test, look_back)
        
        # Reshape
        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))
        
        return {
            "X_train": X_train, 
            "Y_train": Y_train,
            "X_test": X_test, 
            "Y_test": Y_test,
            "scaler": scaler,
            "full_dataset_scaled": dataset
        }